{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Licun Liu  \n",
    "30901235  \n",
    "07-02-2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Streaming application using Spark Structured Streaming (55%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. SparkSession is created using a SparkConf object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkConf class into program\n",
    "from pyspark import SparkConf\n",
    "# Library required\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# local[*]: run Spark in local mode with as many working processors as logical cores on your machine\n",
    "# If we want Spark to run locally with 'k' worker threads, we can specify as \"local[k]\".\n",
    "master = \"local[2]\"\n",
    "# The `appName` field is a name to be shown on the Spark cluster UI page\n",
    "app_name = \"Assignment_3\"\n",
    "# Setup configuration parameters for Spark\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name).set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.0.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell'\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Assignment_3\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define the data schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data schema for the sensor location file\n",
    "schema_2 = StructType([StructField(\"sensor_id\",IntegerType(),True),\n",
    "                     StructField(\"sensor_description\",StringType(),True),\n",
    "                      StructField(\"sensor_name\",StringType(),True),\n",
    "                       StructField(\"installation_date\",DateType(),True),\n",
    "                        StructField(\"status\",StringType(),True),\n",
    "                         StructField(\"note\",StringType(),True),\n",
    "                          StructField(\"direction_1\",StringType(),True),\n",
    "                           StructField(\"direction_2\",StringType(),True),\n",
    "                            StructField(\"latitude\",FloatType(),True),\n",
    "                             StructField(\"longitude\",FloatType(),True),\n",
    "                              StructField(\"location\",StringType(),True)])                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sensor location CSV file into another dataframe\n",
    "df2 = spark.read.csv('Pedestrian_Counting_System_-_Sensor_Locations.csv',dateFormat= 'yyyy/MM/dd' ,schema=schema_2,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Invest the streaming data into Spark Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the topic name from producer\n",
    "topic = \"Assignment\"\n",
    "\n",
    "# Read the stream data\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"127.0.0.1:9092\") \\\n",
    "    .option(\"subscribe\", topic) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the schema of df\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to string format\n",
    "df = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Persist the raw streaming data in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write into parquet files the unsuccessful requests partitioned by status code\n",
    "raw_streaming_data = df.writeStream.format(\"parquet\")\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .option(\"path\", \"parquet/raw_streaming_data\")\\\n",
    "        .option(\"checkpointLocation\", \"parquet/raw_streaming_data/checkpoint\")\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Transformed into the proper formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Create the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the schema\n",
    "schema_1 = StructType([StructField(\"ID\",IntegerType(),True),\n",
    "                     StructField(\"Date_Time\",StringType(),True),\n",
    "                      StructField(\"Year\",IntegerType(),True),\n",
    "                       StructField(\"Month\",StringType(),True),\n",
    "                        StructField(\"Mdate\",IntegerType(),True),\n",
    "                         StructField(\"Day\",StringType(),True),\n",
    "                          StructField(\"Time\",IntegerType(),True),\n",
    "                           StructField(\"Sensor_ID\",IntegerType(),True),\n",
    "                            StructField(\"Sensor_Name\",StringType(),True),\n",
    "                             StructField(\"Hourly_Counts\",IntegerType(),True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Transform the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data\n",
    "df = df.select(F.from_json(F.col(\"value\").cast(\"string\"), schema=schema_1).alias('parsed_value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- parsed_value: struct (nullable = true)\n",
      " |    |-- ID: integer (nullable = true)\n",
      " |    |-- Date_Time: string (nullable = true)\n",
      " |    |-- Year: integer (nullable = true)\n",
      " |    |-- Month: string (nullable = true)\n",
      " |    |-- Mdate: integer (nullable = true)\n",
      " |    |-- Day: string (nullable = true)\n",
      " |    |-- Time: integer (nullable = true)\n",
      " |    |-- Sensor_ID: integer (nullable = true)\n",
      " |    |-- Sensor_Name: string (nullable = true)\n",
      " |    |-- Hourly_Counts: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print out the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Renamed the column\n",
    "The columns need to be renamed appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns\n",
    "df_formatted = df.select(\n",
    "                    F.col(\"parsed_value.ID\").alias(\"ID\"),\n",
    "                    F.col(\"parsed_value.Date_Time\").alias(\"Date_Time\"),\n",
    "                    F.col(\"parsed_value.Year\").alias(\"Year\"),\n",
    "                    F.col(\"parsed_value.Month\").alias(\"Month\"),\n",
    "                    F.col(\"parsed_value.Mdate\").alias(\"Mdate\"),\n",
    "                    F.col(\"parsed_value.Day\").alias(\"Day\"),\n",
    "                    F.col(\"parsed_value.Time\").alias(\"Time\"),\n",
    "                    F.col(\"parsed_value.Sensor_ID\").alias(\"Sensor_ID\"),\n",
    "                    F.col(\"parsed_value.Sensor_Name\").alias(\"Sensor_Name\"),\n",
    "                        F.col(\"parsed_value.Hourly_Counts\").alias(\"Hourly_Counts\")\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Date_Time: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- Mdate: integer (nullable = true)\n",
      " |-- Day: string (nullable = true)\n",
      " |-- Time: integer (nullable = true)\n",
      " |-- Sensor_ID: integer (nullable = true)\n",
      " |-- Sensor_Name: string (nullable = true)\n",
      " |-- Hourly_Counts: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_formatted.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 Timestamp format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with Timestamp format\n",
    "df = df_formatted.withColumn('Date_Time',F.to_timestamp('Date_Time', 'MM/dd/yyyy hh:mm:ss a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Prepare the columns for model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries required\n",
    "import time\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.a - Next calendar date of “Date_Time”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('next_date', df.Date_Time + F.expr('INTERVAL 1 DAYS'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.b - Day of the month information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('next_Mdate',F.dayofmonth(F.col('next_date')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.c - Week of the year information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('next_day_week',F.weekofyear(F.col('next_date')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.d - Day of the week information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.d - Include day of the week information, assuming Monday being the first day of week\n",
    "df = df.withColumn('next_day_of_week',F.dayofweek(F.col('next_date')-F.expr('INTERVAL 1 DAYS')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.e - Rename the column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.e - Rename the column “Hourly_Count” as “prev_count\"\n",
    "df = df.withColumnRenamed('Hourly_Counts','prev_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Predict the next day’s pedestrian count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1 Filter time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter time between 9am-11:59pm\n",
    "df = df.filter(df[\"Time\"] >= 9 ).filter(df[\"Time\"] <= 23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 Load the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the Pipeline Model From the filesystem\n",
    "from pyspark.ml import PipelineModel\n",
    "pipelineModel = PipelineModel.load('count_estimation_pipeline_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\tGBTRegressor_f98f56e12d10-labelCol: Hourly_Counts,\n",
      "\tGBTRegressor_f98f56e12d10-maxBins: 100,\n",
      "\tGBTRegressor_f98f56e12d10-maxDepth: 10\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Print the detail about the model\n",
    "print(pipelineModel.stages[-1]._java_obj.paramMap())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3 Use the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using model to predict\n",
    "predictions = pipelineModel.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4 Write into parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write into parquet files the unsuccessful requests partitioned by status code\n",
    "predict_result = predictions.writeStream.format(\"parquet\")\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .option(\"path\", \"parquet/predict_result\")\\\n",
    "        .option(\"checkpointLocation\", \"parquet/predict_result/checkpoint\")\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Process the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Get the number of hours that the predicted pedestrian count would exceed 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the result exceed 2000\n",
    "predictions = predictions.filter(predictions[\"prediction\"] > 2000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Delete the time in Date_Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to delete the time in Date_Time\n",
    "def func(next_date):\n",
    "    st = str(next_date)\n",
    "    result = st[0:10]\n",
    "    return result\n",
    "\n",
    "#convert to a UDF Function by passing in the function and return type of function\n",
    "udf_func = F.udf(func, StringType())\n",
    "# Call the function\n",
    "df = predictions.withColumn(\"Date_Time\", udf_func(\"Date_Time\"))\n",
    "\n",
    "# Deal with Timestamp format\n",
    "df = df.withColumn('Date_Time',F.to_timestamp('Date_Time', 'yyyy-MM-dd'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Using window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set window size as one day\n",
    "\n",
    "# Convert datetime to date\n",
    "windowedCounts = df \\\n",
    "    .groupBy(F.col('sensor_id'),F.window('Date_Time','1 day'))  \\\n",
    "    .count() \\\n",
    "    .select(\"sensor_id\",F.to_date(F.col('window').end).alias('Next_date'),'count')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.Show the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to show values received from input dataframe\n",
    "def foreach_batch_function(df, epoch_id):\n",
    "    df.show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----+\n",
      "|sensor_id|Next_date|count|\n",
      "+---------+---------+-----+\n",
      "+---------+---------+-----+\n",
      "\n",
      "+---------+----------+-----+\n",
      "|sensor_id|Next_date |count|\n",
      "+---------+----------+-----+\n",
      "|4        |2020-12-02|6    |\n",
      "|2        |2020-12-02|6    |\n",
      "|65       |2020-12-02|2    |\n",
      "|58       |2020-12-02|1    |\n",
      "|67       |2020-12-02|1    |\n",
      "|22       |2020-12-02|3    |\n",
      "|1        |2020-12-02|5    |\n",
      "|24       |2020-12-02|1    |\n",
      "|63       |2020-12-02|3    |\n",
      "|59       |2020-12-02|1    |\n",
      "|68       |2020-12-02|2    |\n",
      "|41       |2020-12-02|5    |\n",
      "+---------+----------+-----+\n",
      "\n",
      "+---------+----------+-----+\n",
      "|sensor_id|Next_date |count|\n",
      "+---------+----------+-----+\n",
      "|68       |2020-12-03|2    |\n",
      "|63       |2020-12-03|1    |\n",
      "|67       |2020-12-03|1    |\n",
      "|22       |2020-12-03|2    |\n",
      "|35       |2020-12-03|4    |\n",
      "|1        |2020-12-03|8    |\n",
      "|69       |2020-12-03|1    |\n",
      "|65       |2020-12-03|1    |\n",
      "|2        |2020-12-03|6    |\n",
      "|59       |2020-12-03|1    |\n",
      "|4        |2020-12-03|6    |\n",
      "|41       |2020-12-03|10   |\n",
      "+---------+----------+-----+\n",
      "\n",
      "+---------+----------+-----+\n",
      "|sensor_id|Next_date |count|\n",
      "+---------+----------+-----+\n",
      "|41       |2020-12-04|11   |\n",
      "|35       |2020-12-04|7    |\n",
      "|2        |2020-12-04|5    |\n",
      "|1        |2020-12-04|7    |\n",
      "|5        |2020-12-04|2    |\n",
      "|4        |2020-12-04|7    |\n",
      "+---------+----------+-----+\n",
      "\n",
      "+---------+----------+-----+\n",
      "|sensor_id|Next_date |count|\n",
      "+---------+----------+-----+\n",
      "|2        |2020-12-05|6    |\n",
      "|47       |2020-12-05|1    |\n",
      "|22       |2020-12-05|3    |\n",
      "|1        |2020-12-05|4    |\n",
      "|4        |2020-12-05|5    |\n",
      "|41       |2020-12-05|6    |\n",
      "|35       |2020-12-05|1    |\n",
      "+---------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show them inside the notebook file\n",
    "query = windowedCounts \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .foreachBatch(foreach_batch_function)\\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .option(\"truncate\",\"false\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.Stop the require"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Write the stream back to Kafka sink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Join the data and select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join these two dataframe\n",
    "df_join = windowedCounts.join(df2,\"sensor_id\",how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select some relevant column\n",
    "df_join = df_join.select(\"sensor_id\",\"Next_date\",\"count\",\"longitude\",\"latitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Construct key and value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct key and value columns\n",
    "df_result = df_join.withColumn('value',F.to_json(F.struct(df_join.columns)))\\\n",
    "            .withColumn('key',F.col('next_date')) \\\n",
    "            .select(F.col('key').cast(\"String\"),F.col('value'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.Send back to Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the stream back to Kafka sink using a different topic name\n",
    "\n",
    "# Topic name is 2.8b_data\n",
    "kafka = df_result.writeStream.format('kafka') \\\n",
    "              .outputMode('update')\\\n",
    "              .option(\"kafka.bootstrap.servers\",\"127.0.0.1:9092\") \\\n",
    "              .option(\"topic\",\"2.8b_data\")\\\n",
    "              .option(\"checkpointLocation\", \"kafka/checkpoint\")\\\n",
    "              .trigger(processingTime = '5 seconds')\\\n",
    "              .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
